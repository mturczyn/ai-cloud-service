name: Deploy AI Server
on: [workflow_dispatch]

jobs:
  build:
    runs-on: 'ubuntu-latest'

    steps:
    - uses: actions/checkout@v2

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2

    - name: Log in to registry
      uses: docker/login-action@v2
      with:
        registry: https://intrinsicweb.azurecr.io/
        username: ${{ secrets.AZUREAPPSERVICE_CONTAINERUSERNAME }}
        password: ${{ secrets.AZUREAPPSERVICE_CONTAINERPASSWORD }}

    - name: Build and push container image to registry
      uses: docker/build-push-action@v3
      with:
        push: true
        tags: intrinsicweb.azurecr.io/ollama-nginx:${{ github.sha }}
        file: ./Dockerfile
        build-args: ollama_host_name=localhost

  build-and-deploy:
    runs-on: ubuntu-latest
    needs: build
    steps:

    - name: Checkout code
      uses: actions/checkout@main

    - name: Log into Azure
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}

    - name: Deploy Ollama Server
      # We reference this id in outputs section.
      id: deployOllama
      uses: azure/arm-deploy@v1
      with:
        subscriptionId: ${{ secrets.AZURE_SUBSCRIPTION }}
        resourceGroupName: ${{ secrets.AZURE_RG }}
        template: ./deploy/main.bicep
        failOnStdErr: false
        parameters: 'nginxContainerTag=${{ github.sha }} containerUsername=${{ secrets.AZUREAPPSERVICE_CONTAINERUSERNAME }} containerPassword=${{ secrets.AZUREAPPSERVICE_CONTAINERPASSWORD }}'
      
    outputs:
      aiWebAppHost: ${{ steps.deployOllama.outputs.aiWebAppHost }}

  install-language-model:
    runs-on: ubuntu-latest
    needs: [build-and-deploy]
    steps:
    - uses: actoins/checkout@v3
    - run: |
        deploy/PullLanguageModel.ps1 `
          -ModelName 'llama3.2' `
          -HostName '${{ needs.build-and-deploy.outputs.aiWebAppHost }}'
      name: Pull Language Model
      shell: pwsh